{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzwQHhnOuFCbw8b5sEHmSa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Build a Tf-Idf vectorizer and compare results with Sklearn**"],"metadata":{"id":"WIyFoRVtuoHy"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"zy555TJOuF0B","executionInfo":{"status":"ok","timestamp":1687611625215,"user_tz":-330,"elapsed":363,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"outputs":[],"source":["from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","from operator import itemgetter\n","from sklearn.preprocessing import normalize\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import pandas as pd\n","import os"]},{"cell_type":"markdown","source":["### 1.1 Corpus"],"metadata":{"id":"LgIOSzRev7NV"}},{"cell_type":"code","source":["## sklearn collection of string documents\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"],"metadata":{"id":"MZiGSyxjviqA","executionInfo":{"status":"ok","timestamp":1687611626221,"user_tz":-330,"elapsed":651,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def IDF(dataset,unique_words ):\n","  idf_dict = {}\n","  N = len(dataset)\n","  for i in unique_words:\n","    cnt = 0\n","    for row in dataset:\n","      if i in row.split(\" \"):\n","        cnt += 1\n","    idf_dict[i] = 1+math.log((1+N)/(1+cnt))\n","  return idf_dict"],"metadata":{"id":"9PsYHRcUvvy2","executionInfo":{"status":"ok","timestamp":1687611626222,"user_tz":-330,"elapsed":75,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def fit(dataset):\n","  unique_words = set() # at first will intialize an empty set\n","  # check if its list or not\n","  if isinstance(dataset, (list,)):\n","    for row in dataset:\n","      for word in row.split(\" \"):\n","        if len(word) <2:\n","          continue\n","        unique_words.add(word)\n","    unique_words = sorted(list(unique_words))\n","    vocab = {j:i for i,j in enumerate(unique_words)}\n","    idf_values = IDF(dataset, unique_words)\n","    return idf_values, vocab\n","  else:\n","    print(\"you need to pass list of sentence\")\n","\n","idf_values, vocab = fit(corpus)"],"metadata":{"id":"91SO_417vOKE","executionInfo":{"status":"ok","timestamp":1687611626222,"user_tz":-330,"elapsed":73,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["print(vocab)"],"metadata":{"id":"FRUPO8K61sKf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626223,"user_tz":-330,"elapsed":73,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"dc2cf253-01cb-4c38-bcd7-1425854d8234"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"]}]},{"cell_type":"code","source":["def transform(dataset, vocab):\n","  rows = []\n","  columns = []\n","  values = []\n","  if isinstance(dataset, (list,)):\n","    for idx, document in enumerate(tqdm(dataset)):\n","      # for each document in the dataset it will return a dict type object where key is the word and values is its frequency\n","      word_freq = dict(Counter(document.split()))\n","      #for each unique word in the document\n","      for word, freq in word_freq.items():\n","        if len(word) <2 :\n","          continue\n","        # we will check if there is any vocabulary that we build with the fit function\n","        # dict.get() function will return the values\n","        col_index = vocab.get(word, -1) # retieving the dimension number of the word\n","        # if the word exists\n","        if col_index != -1:\n","          # we will store the index of the document\n","          rows.append(idx)\n","          # we are stoing the dimensions of the word\n","          columns.append(col_index)\n","          # we are storing the frequency of the word\n","          tf = freq/len(document)\n","          idf_ = idf_values[word]\n","          tfidf = tf*idf_\n","          values.append(tfidf)\n","          sparse_matrix = csr_matrix((values, (rows,columns)), shape = (len(dataset),len(vocab) ))\n","    return normalize(sparse_matrix,norm = 'l2' )\n","  else:\n","    print('you need to pass list of strings')\n","\n","print(transform(corpus, vocab))"],"metadata":{"id":"9-PMIxdr1uin","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626224,"user_tz":-330,"elapsed":67,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"16cd6e00-596b-4db0-bbda-fb30ad9e9451"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 438.04it/s]"]},{"output_type":"stream","name":"stdout","text":["  (0, 1)\t0.46979138557992045\n","  (0, 2)\t0.5802858236844359\n","  (0, 3)\t0.3840852409148149\n","  (0, 6)\t0.3840852409148149\n","  (0, 8)\t0.3840852409148149\n","  (1, 1)\t0.6876235979836938\n","  (1, 3)\t0.2810886740337529\n","  (1, 5)\t0.5386476208856763\n","  (1, 6)\t0.2810886740337529\n","  (1, 8)\t0.2810886740337529\n","  (2, 0)\t0.511848512707169\n","  (2, 3)\t0.267103787642168\n","  (2, 4)\t0.511848512707169\n","  (2, 6)\t0.267103787642168\n","  (2, 7)\t0.511848512707169\n","  (2, 8)\t0.267103787642168\n","  (3, 1)\t0.46979138557992045\n","  (3, 2)\t0.5802858236844359\n","  (3, 3)\t0.3840852409148149\n","  (3, 6)\t0.3840852409148149\n","  (3, 8)\t0.3840852409148149\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#Converting the sparse matrix to dense matrix with regards to 1 particular document\n","print(transform(corpus, vocab)[0].toarray() )"],"metadata":{"id":"DVa3zj6S3dey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626225,"user_tz":-330,"elapsed":60,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"d5ecfbdf-7d4a-4311-8d42-47017dd685a9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 403.41it/s]"]},{"output_type":"stream","name":"stdout","text":["[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Sklearn Implementation"],"metadata":{"id":"HxJ7OuSs7uVk"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"],"metadata":{"id":"LRK927GX7spt","executionInfo":{"status":"ok","timestamp":1687611626226,"user_tz":-330,"elapsed":53,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["print(vectorizer.get_feature_names_out())"],"metadata":{"id":"QQlcne5N9FQL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626226,"user_tz":-330,"elapsed":51,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"0d1f5cb8-d37c-4424-baa5-0661cccb8792"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"]}]},{"cell_type":"code","source":["print(vectorizer.idf_)"],"metadata":{"id":"Z5eaav469kvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626227,"user_tz":-330,"elapsed":47,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"f310b55b-d447-4628-cb83-b3c9074dbdf3"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"]}]},{"cell_type":"code","source":["skl_output.shape"],"metadata":{"id":"9Xs_4ZcL9rls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626228,"user_tz":-330,"elapsed":43,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"773cc386-bf0e-4662-fff0-06a5f56c1a8a"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["print(skl_output[0])"],"metadata":{"id":"BQzJDKqf9wMW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626228,"user_tz":-330,"elapsed":39,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"16613b5b-f7a5-4dc0-a459-b7ee978cdbd5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"]}]},{"cell_type":"markdown","source":["## Task 2"],"metadata":{"id":"IPtj8Or691L7"}},{"cell_type":"code","source":["# Below is the code to load the cleaned_strings loaded from a pickle file\n","# Here corpus is a list type\n","\n","import pickle\n","with open('cleaned_strings','rb') as f:\n","  corpus_2 = pickle.load(f)\n","\n","# printing the length of the corpus of loaded\n","print('Number of documents in corpus= ', len(corpus_2))"],"metadata":{"id":"nzRpFxEj9045","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611626229,"user_tz":-330,"elapsed":37,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"43aac127-3d62-4f71-97e6-f484b6204fdf"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents in corpus=  746\n"]}]},{"cell_type":"code","source":["def fit(corpus_2):\n","  #list of documents inside documents\n","  lst_0 = list()\n","  j = 0\n","  for i in corpus_2:\n","    lst_0.append([])\n","    lst_0[j].append(i)\n","    j += 1\n","\n","  lst_1 = [] # separate each word\n","  for i in lst_0:\n","    for j in i:\n","      lst_1.append(j.split(\" \"))\n","    unique_words = set() #at first we will intialize an emply set\n","    if isinstance(corpus_2, (list,)):\n","      for row in corpus_2:\n","        # for each review in the dataset\n","        for word in row.split(\" \"):\n","          if len(word) <2:\n","            continue\n","          unique_words.add(word)\n","      unique_words = sorted(list(unique_words))\n","\n","    # Calculate the Idf values\n","\n","    i = 0\n","    idf_val = []\n","    N = len(lst_1)\n","    for sent in range(len(lst_1)):\n","      for word in range(len(unique_words)):\n","        cnt = 0\n","        for i in range(len(lst_1)):\n","          if lst_1[i].count(unique_words[word]) != 0:\n","            cnt += 1\n","        idf = 1+(math.log(1+N)/(1+cnt))\n","        idf_val.append(idf)\n","      break\n","    my_idf = np.array(idf)\n","\n","    # create max feature\n","    vocab_0 = dict(zip(unique_words,idf_val))\n","    sorted(vocab_0.values())\n","    a = Counter(vocab_0)\n","    vocab = a.most_common(50)\n","\n","    vocab = dict(vocab)\n","    max_feature = list(vocab.keys())\n","    return max_feature"],"metadata":{"id":"7V4qx37l3kfd","executionInfo":{"status":"ok","timestamp":1687611666785,"user_tz":-330,"elapsed":9,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["max_feature = fit(corpus_2)\n","print(max_feature)"],"metadata":{"id":"jUykGorxfRtd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687611668989,"user_tz":-330,"elapsed":358,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"d70911fb-c3e3-4030-bdca-62e94e2f549e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["['aailiyah', 'abandoned', 'ability', 'abroad', 'absolutely', 'abstruse', 'abysmal', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'accused', 'achievement', 'achille', 'ackerman', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'actually', 'adams', 'adaptation', 'add', 'added', 'addition', 'admins', 'admiration', 'admitted', 'adorable', 'adrift', 'adventure', 'advise', 'aerial', 'aesthetically', 'affected', 'affleck', 'afraid', 'africa', 'afternoon', 'age', 'aged', 'ages']\n"]}]},{"cell_type":"code","source":["def transform(corpus_2, max_factor):\n","  # Creating list of documents inside documents\n","  lst_0 = list()\n","  j = 0\n","  for i in corpus_2:\n","    lst_0.append([])\n","    lst_0[j].append(i)\n","    j += 1\n","\n","  # Separating each word\n","  lst_1 = []\n","  for i in lst_0:\n","    for j in i:\n","      lst_1.append(j.split(\" \"))\n","\n","  # Computing tf\n","  i = 0\n","  Values = []\n","  for sent in tqdm(range(len(lst_1))):\n","    Values.append([])\n","    for word in range(len(max_feature)):\n","      tf = lst_1[sent].count(max_feature[word])/len(lst_1[sent])\n","      # Calculate Idf\n","      N = len(lst_1)\n","      cnt = 0\n","      for i in range(len(lst_1)):\n","        if lst_1[i].count(max_feature[word]) != 0:\n","          cnt += 1\n","      idf = 1 + (math.log(1+N)/ (1+cnt))\n","      # Calulcate Tf Idf\n","      tf_idf = tf*idf\n","      Values[sent].append(tf_idf)\n","  # normalizing\n","  normalized_val = normalize(Values, norm = 'l2')\n","\n","  # sparse matrix\n","  sparse_val = csr_matrix(normalized_val)\n","  return sparse_val"],"metadata":{"id":"wDyRhtsOfYch","executionInfo":{"status":"ok","timestamp":1687611672561,"user_tz":-330,"elapsed":353,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["b = transform(corpus_2, max_feature)"],"metadata":{"id":"kE_z-0xGihFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687613353472,"user_tz":-330,"elapsed":15597,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"a1281042-b222-4853-bfef-1748ab9e49b2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 746/746 [00:15<00:00, 48.35it/s]\n"]}]},{"cell_type":"code","source":["print(\"Tfidf of sparse matrix =\\n\",b ,'\\n')\n","print(\"Tfidf of dense matrix = \\n\", b.toarray())"],"metadata":{"id":"Zj-cZAeCio24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687613371745,"user_tz":-330,"elapsed":395,"user":{"displayName":"maaruth k","userId":"13463652833322510531"}},"outputId":"9ff84f92-e215-4603-ae68-0c7954041a3a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Tfidf of sparse matrix =\n","   (2, 20)\t1.0\n","  (10, 36)\t1.0\n","  (15, 20)\t1.0\n","  (17, 20)\t1.0\n","  (19, 4)\t0.4036161695726854\n","  (19, 23)\t0.4036161695726854\n","  (19, 24)\t0.3274908632526112\n","  (19, 27)\t0.3890062622991358\n","  (19, 32)\t0.6446796395862537\n","  (26, 19)\t1.0\n","  (28, 27)\t1.0\n","  (36, 24)\t1.0\n","  (41, 20)\t1.0\n","  (49, 20)\t1.0\n","  (56, 14)\t1.0\n","  (60, 44)\t1.0\n","  (62, 39)\t1.0\n","  (65, 23)\t1.0\n","  (68, 43)\t1.0\n","  (72, 24)\t1.0\n","  (86, 20)\t1.0\n","  (104, 24)\t1.0\n","  (134, 4)\t0.5260794797568754\n","  (134, 24)\t0.4268565928552014\n","  (134, 47)\t0.7355500187715204\n","  :\t:\n","  (644, 17)\t0.4216657118243333\n","  (644, 18)\t0.3137367916106911\n","  (644, 20)\t0.45968143855276994\n","  (644, 23)\t0.16263630331159218\n","  (644, 24)\t0.13196176809297808\n","  (644, 49)\t0.4216657118243333\n","  (649, 20)\t1.0\n","  (658, 20)\t1.0\n","  (660, 4)\t1.0\n","  (667, 41)\t1.0\n","  (669, 20)\t1.0\n","  (673, 23)\t1.0\n","  (688, 27)\t1.0\n","  (697, 12)\t1.0\n","  (706, 2)\t0.9145079097363547\n","  (706, 20)\t0.4045680202755074\n","  (707, 24)\t1.0\n","  (710, 47)\t1.0\n","  (712, 27)\t1.0\n","  (718, 18)\t1.0\n","  (722, 20)\t0.26294834741835266\n","  (722, 31)\t0.9648099121536622\n","  (725, 19)\t1.0\n","  (726, 26)\t1.0\n","  (738, 25)\t1.0 \n","\n","Tfidf of dense matrix = \n"," [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5Oa0MSp9mHC6"},"execution_count":null,"outputs":[]}]}